{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac34e985",
   "metadata": {},
   "source": [
    "Before we move into Machine-Learning/Predictive Modelling phase, let's take a high-level look at the full process of building a Machine Learning model.\n",
    "\n",
    "**The following steps are involved in the model design:**\n",
    "\n",
    "1. Import libraries\n",
    "2. Import dataset\n",
    "3. Exploratory data analysis\n",
    "4. Data scrubbing\n",
    "5. Pre-model algorithm\n",
    "6. Split the data into Training and Testing sets.\n",
    "7. Pick the suitable Machine Learning algorithm\n",
    "8. Predict\n",
    "9. Evaluate\n",
    "10. Optimize\n",
    "11. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4df1717",
   "metadata": {},
   "source": [
    "### 1. Importing Libraries\n",
    "\n",
    "Python code runs line by line from top to bottom. So if we want to use functions or classes from a package we first need to import the package before we use it. \n",
    "\n",
    "Some people like to import all the required package at the start of the file and on the other hand some people prefer to import the package whenever and wherever they are going to use it within the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bd9dd2",
   "metadata": {},
   "source": [
    "### 2. Import and Read the Dataset\n",
    "\n",
    "Datasets are generally imported from your organization’s records or public repositories such as Kaggle. We can use data from various types of file like `.xlsx, .xls, .csv, .tsv, .sql, .json, .data` etc.\n",
    "\n",
    "**Some examples to understand how to read the data from different file types:** \n",
    "\n",
    "- **pd.read_csv(path_of_file)** - to read .csv, .tsv, .data files\n",
    "- **pd.read_excel(path_of_file)** - to read .xls, .xlsx files\n",
    "- **pd.read_sql(path_of_file)** - to read .sql files.\n",
    "- **pd.read_json(path_of_file)** - to read .json files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f69682",
   "metadata": {},
   "source": [
    "### 3. Exploratory Data Analysis(EDA)\n",
    "\n",
    "The third step, EDA, provides an opportunity to become familiar with your data including distribution, data-types and the state of missing values. Exploratory data analysis also drives the next stage of data scrubbing and deiciding your choice of machine learning algorithms to be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c449843",
   "metadata": {},
   "source": [
    "### 4. Data Scrubbing/Data Wrangling \n",
    "\n",
    "The data scrubbing stage usually consumes the most time and effort in developing a prediction model. This includes `cleaning up the data, inspecting its value, making repairs`, and, ultimately, knowing what to throw and when to throw it out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48bbd5a",
   "metadata": {},
   "source": [
    "### 5. Pre-model algorithm (optional)\n",
    "\n",
    "As an optional extension of the data scrubbing process, unsupervised learning techniques, including k-means clustering analysis and dimensionality reduction algorithms(like PCA, LDA), are sometimes used in preparation for analyzing large and complex datasets.\n",
    "\n",
    "This step, though, is optional and does not apply to every model, particularly for small datasets with a low number of dimensions (features) or rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998a326e",
   "metadata": {},
   "source": [
    "### 6. Splitting the data into training and testing sets\n",
    "\n",
    "We partition the data to train and test sets. It’s also useful to randomize your data at this point using the shuffle feature and to set a random state if you want to replicate the model’s output in the future.\n",
    "\n",
    "Generally the ratios used for this split into `training:testing is 80:20 / 70:30 / 75:25 / 90:10 / 85:15`. The ratio that we pick totally depends on the size of the data but the most commonly use is 80:20 split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cc503a",
   "metadata": {},
   "source": [
    "### 7. Pick the suitable algorithm\n",
    "\n",
    "Choosing a suitable algorithm for the given dataset is very important task and must be done carefully.\n",
    "\n",
    "The algorithm is a mathematical-based sequence of steps and formulas that learns the patterns from the dataset and represent it as a mathematical equation. By executing these series of steps defined by the algorithm, the model looks at input variables in order to interpret patterns, make calculations, and finally produce the output and decisions that we call as `prediction`.\n",
    "\n",
    "As input data is variable(changing), algorithms can produce different outputs based on the input data. Algorithms are also malleable in that they have hyperparameters that can be adjusted to create a more customized model.\n",
    "\n",
    "For context, `the algorithm should not be confused or mistaken for the model. The model is the final state of the algorithm; after hyperparameters are consolidated in response to patterns learned from the data and after a combination of data scrubbing, split validation, and evaluation techniques are completed`.\n",
    "\n",
    "**Some frequently used algorithms are:**\n",
    "\n",
    "1. **Linear Regression:** For Regression data. Used when data is less or moderate in size, it has linear patterns and no missing values or outliers.\n",
    "\n",
    "2. **Logistic Regression:** For Classification data. Used when we have relatively easily separable patterns, limited data with limited outliers.\n",
    "\n",
    "3. **Decision Tree:** For Classification and Regession both. Used when data is large(but with controlled hyperparameters to aviod overfitting).\n",
    "\n",
    "4. **Random Forest:** For Classification and Regression both. Used when data is large, messy and complex. Also used to reduce overfitting issue of decision trees by making many small decision trees instead of one large tree.\n",
    "\n",
    "5. **Gradient Boosting and XGBoost:** For Classification and Regression both. Used when data is large, messy and complex. Used to overcome underfitting issues.\n",
    "\n",
    "6. **Support Vector Machines(SVMs):** Regression and Classification both. Use when data is moderately large, messy and complex. \n",
    "\n",
    "7. **K-Means and Hierarchical Clustering:** Unsupervised Learning algorithms.\n",
    "\n",
    "8. **Multi-layer Perceptrons/Neural Networks:** Both Regression and Classification. When data is very large, too messy and complex. The potential of accuracy in output is very high.\n",
    "\n",
    "9. **Apriori:** For Association Rule mining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd026c0",
   "metadata": {},
   "source": [
    "### 8. Predicting the output\n",
    "\n",
    "After producing an initial model using patterns extracted from the training data, the predict function is called on the test data to validate the performance of the model.\n",
    "\n",
    "The predict function generates a numeric value such as price in regression and in classification, the predict function generates discrete classes, such as the heart disease patient classification, loan approved or not, fraud or not fraud etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f084931d",
   "metadata": {},
   "source": [
    "### 9. Evaluating the performance or accuracy of prediction made by the model\n",
    "\n",
    "The next step of the model design process is to evaluate the results. The evaluation method will depend on the scope of your model. Specifically, this depends on whether it is a classification or regression model. \n",
    "\n",
    "**In classification, the common evaluation methods are the `confusion matrix, classification report, accuracy score and roc-auc score`**.\n",
    "\n",
    "1. **Accuracy Score:** This is a simple metric measuring how many cases the model classified correctly divided by the full number of cases. If all predictions are correct, the accuracy score is 1.0, and 0 if all cases are mispredicted. But is not always the best method, and it does not give us the whole picture. We use accuracy in the case of balanced data and we check few other measures to know more about the actual performance of the model.\n",
    "\n",
    "\n",
    "2. **Precision:** Precision is the ratio of correctly predicted true positives to the total of predicted positive cases. A high precision score translates to a low occurrence of false positives. `Precision = Number of True Positives / Total predicted positives`.  This is the ability of the model not to label a negative case as positive, which is important in the case of drug tests, for example.\n",
    "\n",
    "\n",
    "3. **Recall:** The recall of a model is similar to precision but in this case, represents the ratio of correctly predicted true positives to actual positive cases. In other words, recall addresses the question of how many positive outcomes were rightly classified as positive. This can be understood as the ability of the model to identify all positive cases. Note that the numerator (top) is the same for precision and recall, while the denominators (below) are different. `Recall = No of True Positives / No of actual positive cases.`\n",
    "\n",
    "\n",
    "4. **F1-Score:** F1-score is a weighted average of precision and recall. It’s typically used as a metric for model-to-model comparison rather than for stand-alone model accuracy. In addition, the f1-score is generally lower than the accuracy score due to the way recall and precision are calculated.\n",
    "\n",
    "5. **Sensitivity and Specificity**\n",
    "\n",
    "**In regression, the evaluation methods are:**\n",
    "\n",
    "1. Mean Absolute Error\n",
    "2. Root Mean Squared Error\n",
    "3. R-squared\n",
    "4. Adjusted R-squared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8ff78b",
   "metadata": {},
   "source": [
    "### 10. Optimizing the model performance\n",
    "\n",
    "The final step is to optimize the model. This mean going back to modify the number of clusters or change the hyperparameters of a tree-based learning algorithm like Decision Tree, Random forest etc.\n",
    "\n",
    "We can find optimal hyperparamters manually with trial and error or we can use `GridSearchCV` method to try a bunch of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276a7598",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
