{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fedfa6c",
   "metadata": {},
   "source": [
    "## Machine Learning Workflow\n",
    "\n",
    "There are many steps in the process of creating, implementing and iterating over a machine learning model for a specific data-driven problem. While there is no single universal way of sequencing the different steps that go into a workflow, there are some general principles that are good to follow for optimal performance of a machine learning algorithm.\n",
    "\n",
    "A machine learning workflow has the following steps.\n",
    "1. ETL (Extract, Transform and Load) data\n",
    "2. Data Cleaning\n",
    "3. Train-Test-Validation Split\n",
    "4. EDA (Exploratory Data Analysis)\n",
    "5. Feature Engineering (normalization, removing autocorrelations, discretization, etc.)\n",
    "6. Model Selection and Implementation\n",
    "7. Model Evaluation\n",
    "8. Hyperparameter Tuning\n",
    "9. Model Validation\n",
    "10. Build ML pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed9e57e",
   "metadata": {},
   "source": [
    "#### 1. ETL(Extract, Transform and Load) Data\n",
    "\n",
    "It is often the case that data is stored in a SQL database with a cloud service provider like AWS, Digital Ocean, etc. Depending on the volume of data, an engineer would use a tool like PySpark to extract this data, transform it and load it into a local database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503b1cea",
   "metadata": {},
   "source": [
    "#### 2. Data Cleaning and Aggregation\n",
    "\n",
    "This can involve a range of tasks depending on the form and type of data as well as the problem that the machine learning pipeline is being designed to solve. Some examples include: dealing with null or missing entries, conforming timestamps to a standard, carrying out aggregations like grouping events based on timestamps by the hour or day, grouping IP’s by location, etc. Since Spark is best suited to perform such tasks on big data, this task might very well be the “Transform” part of the above mentioned ETL step. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d213d51f",
   "metadata": {},
   "source": [
    "#### 3. Train-Test-Validation Split\n",
    "\n",
    "Before the modelling phase in Machine Learning, we split our data into Train-Test-Validation sets. The training data is used to train the machine learning models and test data is to test the performance of the trained model. When we take models to productions, then we use the validation data(a part of training data) to tune hyperparameters and/or model validation before we test it on the test data.\n",
    "\n",
    "All the manipulations like scaling, encoding categorical features etc should be done after the splitting of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb1d17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# For feature matrix X and target variable y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45f6b73",
   "metadata": {},
   "source": [
    "#### 4. Exploratory Data Analysis\n",
    "\n",
    "Exploratory Data Analysis or EDA in the context of a machine learning workflow, is the step of inspecting, analyzing and altering your data to get it ready for machine learning modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb90080",
   "metadata": {},
   "source": [
    "#### 5. Feature Engineering\n",
    "\n",
    "Feature engineer refers to prepping, selecting and reducing features in a machine learning problem. This can involve methods that overlap with EDA such as normalization, removing autocorrelations, discretization, etc. Feature engineering can also involve using machine learning algorithms like PCA to reduce dimensionality or methods that are implemented during the model fitting step like regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97e6f61",
   "metadata": {},
   "source": [
    "#### 6. Model Selection and Implementation\n",
    "\n",
    "Now we’re ready to test out different machine learning models. The choice of the model depends on the attributes of the data one’s working with as well as the type of question we’re trying to answer.\n",
    "\n",
    "Refer this [cheatsheet](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html) to choose the right algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d6c475",
   "metadata": {},
   "source": [
    "#### 7. Model Evaluation\n",
    "\n",
    "We’re now getting into the iterative part of the workflow. Whatever model is built, it must be evaluated on the test data. For classification problems, metrics like accuracy, precision, recall, F1 score and AUROC scores indicate how performant the model is and for regression problems, scores like RMSE and R-squared are some commonly used metrics. \n",
    "\n",
    "Machine learning engineers iterate over different types of models to figure out the most optimal model for the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8359fd",
   "metadata": {},
   "source": [
    "#### 8. Hyperparameter Tuning\n",
    "\n",
    "Once a model has been decided upon, it can be tuned for better performance. Hyperparameter tuning is essential in making sure that the model does not overfit or underfit the data.\n",
    "\n",
    "This is key to how well the model is fitting known data and how well it’s able to generalize to new data as well. Hence hyperparameter tuning might be done on the validation or holdout dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e6201a",
   "metadata": {},
   "source": [
    "#### 9. Model Validation\n",
    "\n",
    "Model validation is the process of making sure that the model is still performant on data that it hasn’t seen at all — neither in the training phase nor in the test phase. This can be done either during the hyperparameter tuning step or after. Typically the same metrics used during the model evaluation phase needs to be used here as well so as to make a reasonable comparison with the former."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bb4b32",
   "metadata": {},
   "source": [
    "#### 10. Build ML pipeline!\n",
    "\n",
    "When a machine learning workflow is part of a production cycle, it is often the case that a model is tuned and updated based on incoming information. In other words the model that worked well on last month’s data might not be applicable for this month. It is the job of a Machine Learning Engineer or a Pipeline Engineer to make sure that the model deployed into production is thus flexible and alterable without affecting the rest of the codebase. ML pipelines allow one to do the same!\n",
    "\n",
    "A ML pipeline is a modular sequence of objects that codifies and automates a ML workflow to make it efficient, reproducible and generalizable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d79a72",
   "metadata": {},
   "source": [
    "## Let's get into the implementation of the ML Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df49db4",
   "metadata": {},
   "source": [
    "### Data Cleaning (Numeric)\n",
    "\n",
    "To introduce pipelines, let’s look at a common set of data cleaning/EDA tasks — dealing with missing values and scaling numeric variables. We’re going to convert an existing code base that performs these tasks to more concise code that uses scikit-learn‘s Pipeline using the following steps.\n",
    "\n",
    "1. First, to define a pipeline, we pass a list of tuples of the form (name, transform/estimator) into a Pipeline object. For example, if we wanted to perform imputation with a SimpleImputer first, and scale our numerical variables with a StandardScaler next, the code would look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186f3b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline([(\"imputer\",SimpleImputer()), (\"scale\",StandardScaler())])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60946507",
   "metadata": {},
   "source": [
    "2. Once a Pipeline object has been instantiated, the methods .fit and .transform can be called like we would with any data transformation object in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a366bb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, random_state=0, test_size=0.25)\n",
    "pipeline.fit(x_train)\n",
    "pipeline.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf5a41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(x_train_num)\n",
    "x_transform = pipeline.transform(x_test[num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3791d1d6",
   "metadata": {},
   "source": [
    "If the pipeline includes a machine learning model as well, .predict() can also be called down the line. Each step in the pipeline will be fit in the order provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680dec10",
   "metadata": {},
   "source": [
    "### Data Cleaning (Categorical)\n",
    "\n",
    "We’re now going to implement a task similar to the previous exercise with pipeline.Pipeline(), but with categorical variables now. Specifically, we’ll be dealing with missing values in categorical data and one-hot-encoding categorical variables. We will convert an existing codebase to a pipeline like in the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9155da27",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('imputer', SimpleImputer(strategy = 'most_frequent')), \n",
    "                     ('ohe', OneHotEncoder(drop = 'first', sparse = False))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce4afaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(x_train[cat_cols])\n",
    "pipeline.transform(x_test[cat_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937d6357",
   "metadata": {},
   "source": [
    "### Column Transformer\n",
    "\n",
    "Often times, you may not want to simply apply every function to all columns. If our columns are of different types, we may only want to apply certain parts of the pipeline to a subset of columns. This is what we saw in the two previous exercises. One set of transformations are applied to numeric columns and another set to the categorical ones. We can use scikit-learn‘s ColumnTransformer as one way of combining these processes together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a4ccfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vals = Pipeline([('imputer', SimpleImputer(strategy = 'mean')), ('scale', StandardScaler())])\n",
    "cat_vals = Pipeline([('imputer', SimpleImputer(strategy = 'most_frequent')), \n",
    "                     ('ohe', OneHotEncoder(drop = 'first', sparse = False))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aed1b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the column transformer with the categorical and numerical processes\n",
    "#num_cols in the numerical columns and cat_cols is the categorical columns\n",
    "preprocess = ColumnTransformer(transformers = [('num_preprocess', num_vals, num_cols), \n",
    "                                               ('cat_preprocess', cat_vals, cat_cols)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc975f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess.fit(x_train)\n",
    "preprocess.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5145ea18",
   "metadata": {},
   "source": [
    "### Adding a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc872642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9338a95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"sex\",\"length\",\"diam\",\"height\",\"whole\",\"shucked\",\"viscera\",\"shell\",\"age\"]\n",
    "df = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data\",names=columns)\n",
    "\n",
    "X = df.drop(columns=['age'])\n",
    "y = df.age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55d7068",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = X.select_dtypes(include=np.number).columns\n",
    "cat_cols = X.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562ae3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create some missing values\n",
    "for i in range(1000):\n",
    "    X.loc[np.random.choice(X.index),np.random.choice(X.columns)] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee50baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train-test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, random_state=0, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16da663",
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical and numerical data processing pipelines\n",
    "cat_vals = Pipeline([(\"imputer\",SimpleImputer(strategy='most_frequent')), \n",
    "                     (\"ohe\",OneHotEncoder(sparse=False, drop='first'))])\n",
    "\n",
    "num_vals = Pipeline([(\"imputer\",SimpleImputer(strategy='mean')), (\"scale\",StandardScaler())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d66aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining categorical and numerical pipelines together\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat_process\", cat_vals, cat_cols),\n",
    "        (\"num_process\", num_vals, num_cols)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5fd398",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a pipeline with `preprocess` and a linear regression model, `regr`\n",
    "pipeline = Pipeline([('preprocess', preprocess), ('regr', LinearRegression())])\n",
    "\n",
    "#Fit the pipeline on the training data and predict on the test data\n",
    "pipeline.fit(x_train, y_train)\n",
    "y_pred = pipeline.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b61de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate pipeline score and compare to estimator score\n",
    "pipeline_score = pipeline.score(x_test, y_test)\n",
    "print(pipeline_score)\n",
    "\n",
    "#r-squared score\n",
    "r2_score = r2_score(y_test, y_pred)\n",
    "print(r2_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6fa7ab",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85538157",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Very simple parameter grid, with and without the intercept\n",
    "param_grid = {\n",
    "    \"regr__fit_intercept\": [True,False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e87e88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid search using previous pipeline\n",
    "gs = GridSearchCV(pipeline, param_grid = param_grid, scoring = 'neg_mean_squared_error', cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7e3012",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit grid using training data and print best score\n",
    "gs.fit(x_train, y_train)\n",
    "best_score = gs.best_score_\n",
    "best_params = gs.best_params_\n",
    "print(best_score)\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f612b303",
   "metadata": {},
   "source": [
    "### Final Pipeline\n",
    "\n",
    "We will now be searching over different types of models, each having their own sets of hyperparameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede651ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the `search_space` array from the narrative to add a Lasso Regression model as the third dictionary.\n",
    "search_space = [{'regr': [LinearRegression()], 'regr__fit_intercept': [True,False]},\n",
    "                {'regr':[Ridge()],\n",
    "                     'regr__alpha': [0.01,0.1,1,10,100]},\n",
    "                {'regr':[Lasso()],\n",
    "                     'regr__alpha': [0.01,0.1,1,10,100]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3585e700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a grid search on `search_space`\n",
    "gs = GridSearchCV(pipeline, search_space, scoring='neg_mean_squared_error', cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e005cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best pipeline, regression model and its hyperparameters\n",
    "## Fit to training data\n",
    "gs.fit(x_train, y_train)\n",
    "\n",
    "## Find the best pipeline\n",
    "best_pipeline = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b793ebe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the best regression model\n",
    "best_regression_model = best_pipeline.named_steps['regr']\n",
    "print('The best regression model is:')\n",
    "print(best_regression_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4dd452",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the hyperparameters of the best regression model\n",
    "best_model_hyperparameters = best_regression_model.get_params()\n",
    "print('The hyperparameters of the regression model are:')\n",
    "print(best_model_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d7edc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the hyperparameters of the categorical preprocessing step\n",
    "cat_preprocess_hyperparameters = best_pipeline.named_steps['preprocess'].named_transformers_['cat_preprocess'].named_steps['imputer'].get_params()\n",
    "print('The hyperparameters of the imputer are:')\n",
    "print(cat_preprocess_hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a44bafb",
   "metadata": {},
   "source": [
    "### <font color = 'Blue'>Practical Example combining all the above steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571dc56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.io import arff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7912b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = arff.loadarff('bone-marrow.arff')\n",
    "df = pd.DataFrame(data[0])\n",
    "df.drop(columns=['Disease'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3454812d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert all columns to numeric, coerce errors to null values\n",
    "for c in df.columns:\n",
    "    df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "    \n",
    "#Make sure binary columns are encoded as 0 and 1\n",
    "for c in df.columns[df.nunique()==2]:\n",
    "    df[c] = (df[c]==1)*1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311e72be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calculate the number of unique values for each column\n",
    "print('Count of unique values in each column:')\n",
    "print(df.nunique())\n",
    "\n",
    "# 2. Set target, survival_status,as y; features (dropping survival status and time) as X\n",
    "y = df.survival_status\n",
    "X = df.drop(columns=['survival_time','survival_status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15422abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define lists of numeric and categorical columns based on number of unique values\n",
    "num_cols = X.columns[X.nunique()>7]\n",
    "cat_cols = X.columns[X.nunique()<=7]\n",
    "\n",
    "# 4. Print columns with missing values\n",
    "print('Columns with missing values:')\n",
    "print(X.columns[X.isnull().sum()>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d54e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Split data into train/test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, random_state=0, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c19705",
   "metadata": {},
   "source": [
    "Categorical pipeline will consist of two steps : The first will fill in missing values using the mode and the second will one-hot-encode the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb4a0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Create categorical preprocessing pipeline\n",
    "# Using mode to fill in missing values and OHE\n",
    "cat_vals = Pipeline([(\"imputer\",SimpleImputer(strategy='most_frequent')), (\"ohe\",OneHotEncoder(sparse=False, drop='first', handle_unknown = 'ignore'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660df2d2",
   "metadata": {},
   "source": [
    "Numerical pipeline will consist of two steps:  the first will fill in missing values using the mean and the second will scale features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084f3dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Create numerical preprocessing pipeline\n",
    "# Using mean to fill in missing values and standard scaling of features\n",
    "num_vals = Pipeline([(\"imputer\",SimpleImputer(strategy='mean')), (\"scale\",StandardScaler())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d789b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Create column transformer that will preprocess the numerical and categorical features separately\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat_process\", cat_vals, cat_cols),\n",
    "        (\"num_process\", num_vals, num_cols)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c993ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Create a pipeline with preprocess, PCA, and a logistic regresssion model\n",
    "pipeline = Pipeline([(\"preprocess\",preprocess), \n",
    "                     (\"pca\", PCA()),\n",
    "                     (\"clf\",LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908c0cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Fit the pipeline on the training data\n",
    "pipeline.fit(x_train, y_train)\n",
    "#Predict the pipeline on the test data\n",
    "print('Pipeline Accuracy Test Set:')\n",
    "print(pipeline.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8d21ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Define search space of hyperparameters\n",
    "search_space = [{'clf':[LogisticRegression()],\n",
    "                     'clf__C': np.logspace(-4, 2, 10),\n",
    "                'pca__n_components':np.linspace(30,37,3).astype(int)},\n",
    "                   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd67fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#12. Search over hyperparameters abolve to optimize pipeline and fit\n",
    "gs = GridSearchCV(pipeline, search_space, cv=5)\n",
    "gs.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395c309d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Save the best estimator from the gridsearch and print attributes and final accuracy on test set\n",
    "best_model = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47db76f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Print attributes of best_model\n",
    "print('The best classification model is:')\n",
    "print(best_model.named_steps['clf'])\n",
    "print('The hyperparameters of the best classification model are:')\n",
    "print(best_model.named_steps['clf'].get_params())\n",
    "print('The number of components selected in the PCA step are:')\n",
    "print(best_model.named_steps['pca'].n_components)\n",
    "\n",
    "# 15. Print final accuracy score \n",
    "print('Best Model Accuracy Test Set:')\n",
    "print(best_model.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb5d603",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
